
# ğŸš• **Car detection pipeline**
Many companies are currently applying AI technology for vehicle recognition in buildings to better manage traffic. Our vehicle recognition application uses Modelmesh Kserve, TensorFlow Job distributed training, Kafka, and CI/CD technologies to effectively detect vehicle


# ğŸš€ **Challenge**
This project faced several challenges including ensuring data consistency and scalability during ingestion, managing resources and synchronization in distributed training, automating CI/CD pipelines, converting and deploying models efficiently, ensuring data privacy and security, optimizing performance, and handling the complexities of debugging and troubleshooting in a distributed system.

# ğŸ“• Table Of Contents
- ğŸŒŸ [System Architecture](#system-architecture)
- ğŸ“ [Repository Structure](#repository-structure)
- ğŸ” [How to Guide](#how-to-guide)

# ğŸŒŸ System Architecture
![Pipeline Serving](https://github.com/HungNguyenDev1511/Capstone-Project-Model-Serving/assets/69066161/e86947c5-5e25-4b0b-917d-2b78275dad5f)


# ğŸ“ Repository Structure
ğŸ“¦ Car-detection-serving-model
â”œâ”€ Inference Model Modelmesh 
â”‚  â”œâ”€ README.md
â”‚  â”œâ”€ car-detection-runtime
â”‚  â”‚  â””â”€ triton-servingruntime.yaml
â”‚  â”œâ”€ deployments
â”‚  â”‚  â””â”€ triton-isvc.yaml
â”‚  â”œâ”€ model_repo
â”‚  â”‚  â””â”€ yolov8n_car
â”‚  â”‚     â”œâ”€ 1
â”‚  â”‚     â”‚  â””â”€ model.onnx
â”‚  â”‚     â””â”€ config.pbtxt
â”‚  â”œâ”€ requirements.txt
â”‚  â”œâ”€ triton.yaml
â”‚  â””â”€ utils
â”‚     â”œâ”€ triton_client.py
â”‚     â””â”€ upload_model.py
â”œâ”€ Ingest Data Streaming
â”‚  â”œâ”€ Dockerfile
â”‚  â”œâ”€ README.md
â”‚  â”œâ”€ docker-compose.yml
â”‚  â”œâ”€ kafka_connector
â”‚  â”‚  â””â”€ connect-timescaledb-sink.json 
â”‚  â”œâ”€ produce_json.py
â”‚  â””â”€ run.sh
â”œâ”€ README.md
â”œâ”€ Training Pipeline
â”‚  â”œâ”€ Jenkinsfile
â”‚  â”œâ”€ README.md
â”‚  â”œâ”€ docker-compose.yml
â”‚  â”œâ”€ mlflow
â”‚  â”‚  â””â”€ Dockerfile
â”‚  â””â”€ train
â”‚     â”œâ”€ Dockerfile
â”‚     â”œâ”€ build.sh
â”‚     â”œâ”€ debug.ipynb
â”‚     â”œâ”€ mwt.py
â”‚     â”œâ”€ mwt.yaml
â”‚     â”œâ”€ nets
â”‚     â”‚  â””â”€ nn.py
â”‚     â”œâ”€ utils
â”‚     â”‚  â”œâ”€ config.py
â”‚     â”‚  â”œâ”€ dataset.py
â”‚     â”‚  â””â”€ util.py
â”‚     â””â”€ weights
â”‚        â”œâ”€ model.h5
â”‚        â””â”€ model_s.h5
â””â”€ image
   â”œâ”€ Messenger.png
   â”œâ”€ Mlflow _modelregistry.png
   â”œâ”€ PipelineAllcode.png
   â”œâ”€ Strategy.png
   â”œâ”€ StructureTrainning.png
   â”œâ”€ Structure_Data.png
   â”œâ”€ Topic_tab.png
   â”œâ”€ add_credential.png
   â”œâ”€ add_credential_dockerhub.png
   â”œâ”€ add_token_dockerhub.png
   â”œâ”€ bus.jpg
   â”œâ”€ check_request_github_jenkins.png
   â”œâ”€ connector.png
   â”œâ”€ generate_token_docker_hub.png
   â”œâ”€ get_token_github.png
   â”œâ”€ github_tokens.png
   â”œâ”€ instal_docker_jenkins.png
   â”œâ”€ install_docker_success.png
   â”œâ”€ isvc.png
   â”œâ”€ jenkins_container.png
   â”œâ”€ jenkins_portal.png
   â”œâ”€ jenkins_ui.png
   â”œâ”€ ngrok.png
   â”œâ”€ ngrok_forwarding.png
   â”œâ”€ password_jenkins.png
   â”œâ”€ result.png
   â”œâ”€ result_connect_jenkins_github.png
   â”œâ”€ result_push_dockerhub.png
   â”œâ”€ result_train_pod.png
   â”œâ”€ strategy_scope.png
   â”œâ”€ train_process.png
   â”œâ”€ ui_build_jenkins.png
   â”œâ”€ validate_connect_repo.png
   â””â”€ webhook_github.png


# ğŸ” How to Guide


That is a Project about Serving one model using Modelmesh Kserve and using technique multiple training you can follow all components to see all pipelines The idea of this pipeline will be described below:
- Step 1: Ingest Data from the end user and send directly to PostgreSQL. I use Kafka to send data to PostgreSQL, ready for training in Step 2
- Step 2: In this step, I use all data in PostgreSQL for the training job and use Tensorflow Job and K8s to create multiple Vmware for training, the result will be the weight file after training and you can use this for Serving Step. The Idea in this step is, that the data is countinuously ingested into the database and you can use a CICD tool like Jenkins or something like that to conduct continuously the continuous job
- Step 3: In this step, I use the model file in the training step to serve, should be converted to onnx model platform and I can serve to Modelmesh