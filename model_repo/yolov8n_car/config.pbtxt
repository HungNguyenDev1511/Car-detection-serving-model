# Model configuration file (optional)
# https://github.com/triton-inference-server/tutorials/blob/main/Conceptual_Guide/Part_1-model_deployment/README.md#model-configuration
name: "yolov8n_car" 
backend: "onnxruntime" # Select the backend to run the model https://github.com/triton-inference-server/backend#where-can-i-find-all-the-backends-that-are-available-for-triton
max_batch_size : 2 # Max batch size the model can support
# In most cases, Triton can help to extract `input` and `output`
# but we should declare it explicitly
input [
  {
    name: "images"
    data_type: TYPE_FP32
    dims: [ 3, 640, 640 ] # If no batch, pls use [ 1, 640, 640 ]
  }
]
output [
  {
    name: "output0"
    data_type: TYPE_FP32
    dims: [ -1, -1 ] # If no batch, pls use [ 84, 8400 ]
  }
]

instance_group [ { kind: KIND_CPU } ]